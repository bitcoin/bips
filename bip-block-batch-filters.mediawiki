<pre>
  BIP: Block Batch Filters for Light Clients
  Layer: Peer Services
  Title: Block Batch Filters for Light Clients
  Author: Aleksey Karpov <admin@bitaps.com>
  Comments-Summary: None yet
  Comments-URI: 
  Status: Draft
  Type: Standards Track
  Created: 2019-09-20
  License: CC0-1.0
</pre>


== Abstract ==

This BIP describes a new filter types on blocks data, for use in the BIP 157 light client protocol. 
The filter construction proposed is an alternative to BIP 158 filters, with lower false positive rate
and block filters aggregated to batches. For compression  using Delta-Huffman coding. This document 
defines new types of filters that vary in batch size and separated by address types.

== Motivation ==

[[bip-0158.mediawiki|BIP 158]] defines the initial filter type basic that is designed to reduce the filter 
size for regular wallets and minimize the expected bandwidth consumed by light clients, downloading 
filters and full blocks.

==== False positive rate is low ====

To achieve privacy and security, wallets must follow the rule - no address reuse. This means that each payment
received or sent uses one or more new addresses. The wallet makes 2-3 payments per day, during the year the number 
of used addresses increases to about 1000 pieces. All addresses that were used at least once should be monitored
for new payments. Assume in this BIP that a reasonable estimated number of wallet monitoring addresses is 10000 pieces. 

False positive rate in BIP 158 is low. We can achieve lower bandwidth consumption with higher false positive rate filte, 
while sync blockchain. Below provided result of test with  10000 addresses in monitoring for 2000 blocks (7300 addresses each):

<pre>
False positive blocks 32 from 2000 
Filters size:  36.64 Mb
Downloaded 68.64 Mb (32 blocks included);
</pre>

Size of downloaded false positive blocks (~32 Mb) can be excluded by use filters with lower false positive rate . 

<pre>
False positive blocks 0 from 2000
Download 57.64 Mb  
Filters size:  57.64 Mb
</pre>

Bandwidth consumption reduced about 16%, good result, but is not enough to be reasonable argument to 
set lower false positive rate and increase filter size.

==== Total filters size and filters batching ====

Total filters size for BIP 158 is about 4.5 Gb. It's not so much for a full node, but for light client we 
need size smaller as possible (to store permanently or download one time for initial sync).

To achieve best level of privacy and security wallets must follow the rule “no address reuse”,
but despite the recommendations “no address reuse”, a lot of addresses was reused and will be reused in future. 
About 94% of all addresses recorded in bitcoin blockchain have zero balance at this moment, this means that at 
least 2 records for 94% of addresses in blockchain. Each spent utxo have 1 creating and 1 spending records.  
Combining filters for block ranges, excludes all reused and spent records as duplicates in a filter within 
the range of combined blocks.

Block combined filters with set of elements about 5 000 000 pieces per batch reduce total filters size to 2.3 Gb.

Block combined  filters with set of elements about 20 000 000 pieces per batch reduce total filters size to 1.6 Gb.

The increase set elements limit in batch  more then 20 000 000 pieces no longer gives a tangible effect. 

Reducing downloadable total filter size to 1.6 Gb is obviously good improvement for light clients and bitcoin scalability.


==== Batching and BIP 158 ====

BIP 158 use siphash as hash function to produce set of filter elements. Siphash is effective function computes 64-bit 
or 128-bit values from a variable-length message and 128-bit secret key. For a 128-bit secret key, the first 128-bit of 
block hash is used. Consequently the same element but in different blocks has a different hashes. As a result of this
we have to recalculate all hashes for monitoring elements for each block and we can’t aggregate block filters sets  to batch. 

SipHash is fundamentally different from cryptographic hash functions like SHA and main goal is a produce message 
authentication code: a keyed hash function like HMAC. To create a probabilistic filter structure, an effective hash 
function with good uniform distribution is needed. MAC functionality is out of block filters task scope. Using a constant
value of 128-bit key allows using sighash as a regular hash function and opens up the opportunity for filters batching.

The 64-bit SipHash outputs are then mapped uniformly over the desired range by multiplying with F and taking the top 
64 bits of the 128-bit result. Where F = N * M, N - count of filter elements, M - inverse of  probability for set of 
elements matches other items.  This algorithm is a faster alternative to modulo reduction, as it avoids the expensive 
division operation. In other words, 64-bit SipHash outputs are mapped to values with a given number of bits 
to achieve a given false positive rate. To archive shotter filter size with Golomb-Rice coding N should be 
exactly match the count of elements in filter. This means that filters with fewer elements cut out more bits from hashes 
than filters with more elements. As a result, we cannot do batching for filters due to the lack of reduced bits and 
still have to recalculate all hashes for wallet monitoring elements for each block.

In case we use static parameters for Golomb-Rice coding F, with necessary bits and false positive rate for all blocks 
independently from block filter elements count. Redundancy size grows significantly for the filters with the count 
of elements moving away from the maximum count.

In this BIP describes alternative filter coding algorithm based on combination of Deltas coding and double Huffman coding, 
which allows to minimize the size redundancy for filters with a small number of elements and approaches the compression up
to 98,3 % from optimal,  for filters with an average and a large number of elements.


== Specification ==

For each block, filters are derived containing sets of addresses or scripts associated with the block. 

At a high level, a batch filter is constructed from a set of <code>N</code> items by:
* hashing all items to 64-bit integers in the range <code>[0, F)</code>
* sorting the hashed values in ascending order and remove duplicates
* apply Deltas coding - computing the differences between each value and the previous one
* writing the differences sequentially to bit string with minimal bits required to encode each value
* writing separate bit string with number of bits for corresponding value in first bitstring
* compress second bit string with 2 round Huffman coding 

==== Hashing data objects ====

The first step in the filter construction is hashing the variable-sized raw items in the set to the range <code>[0, F)</code>, where <code> F = N * M</code>.  Currently about 560 millions unique addresses stored in bitcoin blockchain, reasonable values of <code> N, M </code>  defined in this BIP is <code> N = 10 000 </code> (count of unique addresses monitored by wallet), <code> M = 1 000 000 000 </code> (count of unique addresses in blockchain). This parameters may be increased in the future with the growth of the blockchain.

The items are first passed through the pseudorandom function SipHash, which takes a 128-bit key <code> k = 0 </code> and a variable-sized byte vector and produces a uniformly random 64-bit output. Implementations of this BIP MUST use the SipHash parameters <code>c = 2</code> and <code>d = 4</code> for best performance.

The 64-bit SipHash outputs are then mapped uniformly over the desired range by multiplying with <code>F</code> and taking the top 64 bits of the 128-bit result. This algorithm is a faster alternative to modulo reduction, as it avoids the expensive division operation, the same as in BIP 158. 

==== Deltas coding ====

Instead of writing the items in the hashed set directly to the filter, greater compression is achieved by only writing the differences between successive items in sorted order.

==== Huffman coding ====

Instead of encoding values into a sequence of bits with a prefix that allows decoding, we divide the data into 2 separate sequences.  First sequence with  element values written  as a string of bits using the minimum required number bits for each element. Second sequence with number of bits written in first sequence for each element, written as byte string. As result we got first sequence without redundancy and second one with a lot of duplicate values. 

Huffman encoding is effective algorithm to encode duplicate values in sequence. 2 round of huffman encoding on second sequence give optimal result. To reduce size of compressed second sequence we will use minimal  bits threshold for writing values for first sequence. Count of bits to encode most of item values is in range 20 - 24, we apply 20 bit minimal threshold to reduce huffman coding dictionary, get more duplicates in second sequence and get better huffman compression result.

<pre>
def encode_dhcs(elements, min_bits_threshold=20):
   # Delta-Hoffman coded set
   data_sequence = bitarray()
   data_sequence_append = data_sequence.append

   deltas_bits = deque()
   deltas_bits_map_freq = dict()
   last = 0

   for value in  sorted(elements):
       delta =  value - last

       bits = delta.bit_length()
       if bits < min_bits_threshold:
           bits =  min_bits_threshold

       deltas_bits.append(bits)

       try:
           deltas_bits_map_freq[bits] += 1
       except:
           deltas_bits_map_freq[bits] = 1

       while bits > 0:
           data_sequence_append(delta & (1 << (bits - 1)))
           bits -= 1
       last = value

   # huffman encode round 1
   # encode bits length sequence to byte string
   codes_round_1 = huffman_code(huffman_tree(deltas_bits_map_freq))
   r = bitarray()
   r.encode(codes_round_1, deltas_bits)
   bits_sequence = r.tobytes()
   bits_sequnce_len_round_1 = r.length()

   # huffman encode round 2
   # encode byte string
   deltas_bits = deque()
   deltas_bits_map_freq = dict()
   for i in bits_sequence:
       b = i >> 4
       c = i & 0b1111
       deltas_bits.append(b)
       try:
           deltas_bits_map_freq[b] += 1
       except:
           deltas_bits_map_freq[b] = 1

       deltas_bits.append(c)
       try:
           deltas_bits_map_freq[c] += 1
       except:
           deltas_bits_map_freq[c] = 1

   codes_round_2 = huffman_code(huffman_tree(deltas_bits_map_freq))
   r = bitarray()
   r.encode(codes_round_2, deltas_bits)
   bits_sequnce_len_round_2 = r.length()
   bits_sequence = r.tobytes()


   code_table_1 = int_to_var_int(len(codes_round_1))
   for code in codes_round_1:
       code_table_1 += int_to_var_int(code)
       code_table_1 += int_to_var_int(codes_round_1[code].length())
       code_table_1 += b"".join([bytes([i]) for i in codes_round_1[code].tolist()])

   code_table_2 = int_to_var_int(len(codes_round_2))
   for code in codes_round_2:
       code_table_2 += int_to_var_int(code)
       code_table_2 += int_to_var_int(codes_round_2[code].length())
       code_table_2 += b"".join([bytes([i]) for i in codes_round_2[code].tolist()])


   d_filter_len = data_sequence.length()
   d_filter_string = data_sequence.tobytes()

   return  b"".join((code_table_1,
                     code_table_2,
                     int_to_var_int(bits_sequnce_len_round_1),
                     int_to_var_int(bits_sequnce_len_round_2),
                     bits_sequence,
                     int_to_var_int(d_filter_len),
                     d_filter_string))

</pre>

<pre>
def decode_dhcs(h):
   # Delta-Hoffman coded set
   stream = get_stream(h)

   # read code_table_1
   c = var_int_to_int(read_var_int(stream))
   code_table_1 = dict()
   for i in range(c):
       key = var_int_to_int(read_var_int(stream))
       l = var_int_to_int(read_var_int(stream))
       code =  bitarray([bool(k) for k in stream.read(l)])
       code_table_1[key] = code

   # read code_table_2
   c = var_int_to_int(read_var_int(stream))
   code_table_2 = dict()
   for i in range(c):
       key = var_int_to_int(read_var_int(stream))
       l = var_int_to_int(read_var_int(stream))
       code =  bitarray([bool(k) for k in stream.read(l)])
       code_table_2[key] = code

   # read compressed deltas
   deltas_bits_len_1 = var_int_to_int(read_var_int(stream))
   deltas_bits_len_2 = var_int_to_int(read_var_int(stream))
   deltas_byte_len = deltas_bits_len_2 // 8 + int(bool(deltas_bits_len_2 % 8))

   r = stream.read(deltas_byte_len)
   deltas = bitarray()
   deltas.frombytes(r)

   while deltas.length() > deltas_bits_len_2:
       deltas.pop()

   # Huffman decode round 1
   r = deltas.decode(code_table_2)
   deltas_string = bytearray()
   for i in range(int(len(r)/2)):
       deltas_string += bytes([(r[i*2] << 4) + r[i*2 + 1]])

   # Huffman decode round 2
   r = bitarray()
   r.frombytes(bytes(deltas_string))

   while r.length() > deltas_bits_len_1:
       r.pop()

   deltas_bits = r.decode(code_table_1)


   d_filter_bit_len = var_int_to_int(read_var_int(stream))
   d_filter_byte_len = d_filter_bit_len // 8 + int(bool(d_filter_bit_len % 8))
   r = stream.read(d_filter_byte_len)

   d_filter = bitarray()
   d_filter.frombytes(r)

   while d_filter.length() > d_filter_bit_len:
       d_filter.pop()

   f = 0
   f_max = d_filter.length()
   decoded_set = set()
   last = 0

   for bits in deltas_bits:
       d = 0
       while bits  > 0 and f < f_max :
           bits -= 1
           d = d << 1
           if d_filter[f]:
               d += 1
           f += 1
       last += d
       decoded_set.add(last)
      
   return decoded_set

</pre>

== Block Batch Filters ==

This BIP defines list of  filter types:
* block 
  <code> P2PK+P2PKH  - 0x01, P2SH  - 0x02, P2WPKH  - 0x03, P2WPSH  - 0x04 </code>
* batch level 1
  <code> P2PK+P2PKH  - 0x05, P2SH  - 0x06, P2WPKH  - 0x07, P2WPSH  - 0x08 </code>
  <code> size - 2 500 000 unique addresses + addresses till end of last block in ran </code>
* batch level 2  
  <code> P2PK+P2PKH  - 0x09, P2SH  - 0x0A, P2WPKH  - 0x0B, P2WPSH  - 0x0C </code>
  <code> size - 10 000 000 unique addresses + addresses till end of last block in range </code>
